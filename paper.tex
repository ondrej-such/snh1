\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{jmlr2e}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{rotating}

\newtheorem{prop}{Proposition}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\DeclareMathOperator*{\divm}{div}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{Sample JMLR Paper}{One and Two}
\firstpageno{1}

\begin{document}

\title{Comparison of canonical polychotomous coupling methods with the method of Wu-Lin-Weng}

\author{\name Ondrej Šuch \email ondrejs@savbb.sk \\
       \addr Matematický ústav SAV\\
       Ďumbierska 1\\
       Banská Bystrica, 974 01, Slovakia
       \AND
       \name Peter Novotný \email two@cs.berkeley.edu \\
       \addr Division of Computer Science\\
       University of California\\
       Berkeley, CA 94720-1776, USA}

\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\blindtext
\end{abstract}

\begin{keywords}
  keyword one, keyword two, keyword three
\end{keywords}

\section{Introduction}

%\blindmathpaper

Multi-class classification is considered to be a more challenging problem than binary classification.  A natural approach to multi-class classification is to reduce it to a series of binary classification problems and deduce the multi-class decision by aggregating decisions for the binary problems. 

One popular method belonging to  this paradigm uses probabilistic modelling for one-vs-one series of binary problems. Notably, it is used as the basis for multi-class classification models using support vector machines (SVM) in LIBSVM library. However one can use the approach more generally,  because it can be applied to any probabilistic binary classification method. 

A key step in probabilistic one-vs-one modelling is the aggregation of results of individual binary classifiers - so-called \emph{coupling method}. The most commonly used coupling method is that proposed by Wu-Lin-Weng. As noted in \cite{dohau}, the method is one of several non-canonical decisions used in multi-class SVM modelling. The goal of this paper is to compare its behavior with canonical methods. 


\section{Theoretical basis of coupling methods}

Suppose we have $K$ classes of objects, each distributed according to a probability distribution $p_i$, $i=1,\ldots, K$ on a feature space $X$. In a multi-class classification problem we are provided with the prior probabilities $\Pi = (P_1, \ldots, P_k)$ of each class. Given $x$ in $X$ we aim to find the probability that $x$ belongs to $i$-th class. 

An exact solution to this problem is provided by the Bayes classifier. To simplify the discussion we will assume that $p_i(x) > 0$ for all $x$ and $P_i = 0$ for all $i$. Commonly used classifiers such as linear discriminant analysis, (penalized) logistic regression or support vector machines The Bayes classifier predicts that the probability of $x$ belonging to the $i$-th class is
\begin{align}
 p^\Pi_i = \frac{p_i(x) P_i}{\sum_{k=1}^K p_k(x)P_k}.
\end{align}

Besides the multi-class classifier we may also consider the binary Bayes classifiers for any pair $(i,j)$ of classes. Assuming prior on the two classes is proportional to $(P_i, P_j)$, the binary Bayes classifier predicts the probability of $i$-th class as 

\begin{align}
	r_{ij}(x) = \frac{p_i(x) P_1}{p_i(x)P_1 + p_j(x)P_2}.
\end{align}

Knowledge of vector $\boldsymbol{p}^\Pi$ is essentially equivalent to knowing the function $r_{ij}(x)$. Indeed, if $p_{i}> 0$ for all $x$, then we have
\begin{align}
r_{ij}(x) = \frac{p_i^\Pi}{p_i^\Pi + p_j^\Pi}. \label{eq:bt1}
\end{align}

Conversely, it is easy to show that  the system of equations \label{eq:bt1} has a unique solution.

\begin{prop} \label{prop:binary2multi}
The system of linear equations \eqref{eq:bt1}  has a unique solution which is the multi-class Bayes classifier.
\end{prop}
\begin{proof}
Indeed, we have 
\begin{align*}
\frac{1}{r_{ij}(x)} - 1 = \frac{p_j^\Pi}{p_i^\Pi}
\end{align*}
\end{proof}


\subsection{One-vs-one classification }

Suppose that $K$ classes $C_1, \ldots, C_K$ are distributed according to probability distributions $p_i$ on a  space $X$. Let us denote the $r_{ij}(x)$ the output of Bayes (binary) classifier for $x$ in $X$ yielding the probability $p(x \in C_i \mid x\in C_i \cup C_j )$. Then we have
\begin{equation}
r_{ij}(x)= \frac {p_i(x)}{p_i(x) + p_j(x)}. \label{eq:bt1}
\end{equation}


The system of equations is called Bradley--Terry equations.  The equations in \eqref{eq:bt1}  can be transformed to a system of linear equations. The theoretical basis for one-vs-one classification framework is provided by the following result.



Suppose we have estimators $\hat r_{ij}$ for Bayes predictions $r_{ij}$ for all pairs $i\not= j$. A \emph{coupling method} obtains a multi-class probabilistic estimate $\boldsymbol{\hat p}= (\hat p_1, \ldots, \hat p_K)$ that satisfy
\begin{equation}
	\hat{r}_{ij}(x) \approx \frac {\hat p_i(x)}{\hat p_i(x) + \hat p_j(x)}. \label{eq:bt2}
\end{equation}

 One should note that 
\begin{itemize}
\item the resulting system of equations will usually not be consistent, because $\hat{r}_{ij}$ are only estimates of true values $r_{ij}$,
\item when probabilities are parametrized in other ways (e.g. other common parametrizations are as odds, or log-odds), the system of equations \eqref{eq:bt} is non-linear.
\end{itemize}

\subsection{Exact desiderata on coupling method}

Let us outline what kind of conditions may be desirable in a coupling method.

%\begin{itemize}
%\item a coupling method should should always provide a unique solution, at least when all $r_{ij}>0$,
%\item Bradley-Terry consistency,
%\item canonicity,
%\item symmetry,
%\item Bayes covariance,
%
%\item good accuracy on benchmark tasks,
%\item lack of parameters, or at least having a small number of parameters,
%\item Hinton's minimality,
%\item simplicity of computation.
%\end{itemize}



\emph{Unique solution} refers to the fact that the procedure to compute a multi-class probabilistic prediction always converges to a unique solution. One way to satisfy this condition would be that it amounts to a linear system of equations in $K$ variables, and the matrix of the system would be invertible. Another example would be when the procedure amounts to finding the minimum of a strongly convex function. 

\emph{Bradley-Terry consistency} refers to the outcome of a procedure providing a unique solution. The requirement states that should we apply the coupling procedure to binary Bayes classifiers, then the result is the output of multi-class Bayes classifier i.e. satisfies \eqref{eq:bt1}.

\emph{Canonicity} refers to requirement that the method should be in a natural way unique. Borrowing an example from regression analysis, Gauss-Markov theorem states that ordinary least squares (OLS) estimates is best linear unbiased estimate of the coefficients of a linear model. Therefore OLS  is a \emph{canonical} way to estimate parameters of a regression model. Note that potentially there may be multiple canonical methods, derived from (or satisfying) differing sets of assumptions. 

\emph{Symmetry} refers to the action of the permutation group which acts both on the classes as well as on predictions. The action of a permutation $\sigma$ extends to the action on pairwise matrix by 
\begin{equation}
	\hat r^\sigma_{i,j} = \hat r_{\sigma(i), \sigma(j)}
\end{equation}

\emph{Bayes covariance} is a notion introduced in work \cite{BarredaSuch}. It refers to the behavior of a probabilistic multi-class method built in one-vs-one fashion, when the priors change. Any Bayes classifier changes its prediction when the priors change. The change happens according to Bayes theorem.


\subsection{Inexact desiderata for coupling methods}

\emph{Good accuracy on benchmark tasks} is  a strong argument to prefer a particular coupling method. This criterion is of course subjective, because there is a great variety of datasets in practice and inevitably there will be instances where any coupling method would underperform. However a good coupling method should perform robustly across a variety of datasets. A historical  example to follow is linear discriminant (LDA) of  Sir Ronald Fisher. At the time he was submitting his paper describing LDA, he had confirmed usefulness of his method not only on the famous iris dataset, but also on two separate collaborations with archaeologists. 

\emph{Lack of parameters} is strongly beneficial for situations when the dataset is small and it would be imprudent to reserve a sizeable portion to obtain unbiased estimates of the parameters of the coupling method. This issue gains expediency when the underlying binary classification method requires crossvalidation to select some hyperparameters. For example, this is the case of support vector machines, where often one opts for RBF kernel, which requires choosing two hyper-parameters. 

\emph{Simplicity of computation}  may refer to the computational requirement for inference. With the dramatic increase of edge computing devices, such as smartphones, a simple, fast, energy-efficient algorithm may be preferable to a better performing one which would require more computational resources. 

Let us finally describe \emph{Hinton's minimality}. Geoffrey Hinton is reported to object to the underlying principle of one-vs-one classification, where any single classifier influences the multi-class prediction \cite[p.~467]{hastie1998classification}. Suppose we are classifying objects from CIFAR-10 dataset.  If the true class is  a ``dog'', why should we care what is the output of the classifier distinguishing ``airplanes'' from ``ships''? The binary classifier is trained only on airplanes and ships, and has never seen a dog. One may thus prefer a classifier which somehow suppresses noisy output of (seemingly) irrelevant classifiers.


The first that the five conditions have a clear mathematical definition, whereas the rest are not exact. For instance while the ideal situation for a coupling method is to have no parameters, the presence of one or two trainable parameters may be justified, if it yields noticeably better performance.


\section{Examples of coupling methods} \label{sec:coupling}

There is a plethora of methods available for coupling. Let us describe four major categories.




\subsection{Optimizing binary divergence}

A multi-class estimate $\hat{\boldsymbol{p}}$ obtained by a coupling method implies via \eqref{eq:bt1} probability distributions on each pair of classes:
\begin{align*}
	\hat D_{ij}^\textrm{multi}= \biggl(\frac{\hat{p_i}}{\hat p_i + \hat p_j},\frac{\hat{p_j}}{\hat p_i + \hat p_j}\biggr)	
\end{align*}
On the other hand, binary classifiers provide another distribution for each pair of  classes, namely
\begin{align*}
\hat D_{ij}^\textrm{binary} = (\hat r_{ij}, \hat r_{ji}).
\end{align*}

A natural way to find  $\hat{\boldsymbol{p}}$ is thus to minimize a functional incorporating divergence measures between these distributions
\begin{align*}
\hat{\boldsymbol{p}} = \argmax_{\boldsymbol{p}} \sum_{ij} w_{ij} \divm (\hat D_{ij}^\textrm{multi}, \hat D_{ij}^\textrm{binary})
\end{align*}

A notable example which uses this principle is the coupling method introduced in \cite{hastie1998classification}. Their method uses Kullback-Leibler divergence $\divm_\textrm{KL}$
\begin{align*}
	\divm\nolimits_\textrm{KL} (\boldsymbol{p}, \boldsymbol{q})= \sum p_i \log (p_i/ q_i)
\end{align*}

Kullback-Leibler divergence strongly penalizes  mispredictions when the true class is predicted to have very low probability. In the context of classification, when we are primarily concerned with accuracy , one could advantageously use other Bregman divergence derived from a proper score.
 
For instance, a popular alternative is divergence arising from the quadratic  score (also called Brier score).  This divergence takes the form
$$
\divm\nolimits_\textrm{quad} (\boldsymbol{p}, \boldsymbol{q})= \sum (p_i - q_i)^2.
$$
Unlike Kullback-Leibler divergence, the penalty for incorrect prediction with arbitrarily low probability is bounded (by one). This quadratic score is the basis of the popular Wu-Lin-Weng's method \cite{wu2004probability}. 

Their method use an additional trick, which we term \emph{self-referentiality}. In the method of Hastie and Tibshirani, $w_{ij}$ are constants derived from observed class frequencies. However, we could use weights $w_{ij}$ that increase when the predicted probability of either class $i$ or $j$ is high. A reasonable choice would be for instance $w_{ij}= p_i + p_j$. In the case of quadratic score, it is more convenient to use weights $w_{ij} = (p_i + p_j)^2$ because then the resulting optimization problem is just minimization of a quadratic form
\begin{align*}
\hat{\boldsymbol{p}} = \argmin_{\boldsymbol{p}} \sum_{i,j} (r_{ij}p_j - r_{ji}p_i)^2.
\end{align*}

We note that the method is symmetric, but the choice of weights $w_{ij}$ to make the method computationally amenable is somewhat arbitrary. Thus we cannot view Wu-Lin-Weng's method as canonical.

\subsection{Arboreal-based methods}


Given an overdetermined system of equations, a common approach is to select a minimal subset of equations needed to solve for the unknowns. This principle underlies arboreal coupling methods.

An arboreal coupling method ignores all but $K-1$ of the pairwise predictions. A simple example for 3 classes is the classifier which ignores prediction of the binary classifier distinguishing between classes 1 and 3. The two equations form \eqref{eq:bt2} together with the requirement that the prediction probabilities should sum to 1 yield the following system of equations
\begin{equation}
	\begin{split}
		\frac{\hat p_1}{\hat p_2} &= \vec{r}_{12}\\
		\frac{\hat p_2}{\hat p_3} &= \vec{r}_{23}\\
		\hat p_1 + \hat p_2 + \hat p_3 &= 0
	\end{split}
\end{equation}

For $K=3$, there are three different arboreal coupling methods, which we shall call $omit12$, $omit13$, $omit23$. 

In order to obtain a regular system of equations for  larger values of $K$ , the resulting classifiers must represent a connected tree within the complete graph on all classes. Any arboreal method is non-canonical, because there is no natural way to choose a tree in a complete graph.

\subsection{Methods emphasizing Hinton's minimality}

A variant on arboreal methods is the following oracle method which we shall call Hinton's oracle. It  assumes it has the access to the ground truth $y = C_i$. It outputs probabilities by solving only the equations
$$
\frac{p_i}{p_j} = \vec{r}_{ij},\quad \textrm{for }i\not = j,
$$
together  with total sum equation
$$
\sum_{i=1}^K  \hat p_i = 1.
$$

For any sample, its result is identical to the arboreal method where the underlying tree forms a star. This method is clearly the optimal one with respect to Hinton's minimality criterion, because it ignores the output of any classifier not trained on the true class. It is not a true classification method, however, because it ``peeks'' at the label. Its main utility is to provide a way to benchmark other methods with respect to Hinton's minimality.

In order to convert Hinton's oracle into a true classification method we can use the principle of self-referentiality.  Denote by $\hat{\boldsymbol{T}}_i$ the prediction of the arboreal method, where the underlying tree is a star centered at class $C_i$. The predictions will generally span the simplex $\sum_i \hat p_i = 1$. In that case the prediction $\hat{\boldsymbol{p}}$ of the method  can be expressed as a linear combination of $\hat{\boldsymbol{T}}_i$. Let us now define \emph{radial} coupling method. The  method requires that for  weights we could in fact use the components of $\boldsymbol{p}$ i.e.
\begin{equation}
	\hat {\boldsymbol{p}} = \sum_{i=1}^K p_i \hat{\boldsymbol{T}}_i. \label{eq:radial}
\end{equation}
%
The intuition behind \eqref{eq:radial} is that if some component of $\hat{\boldsymbol{p}}$ is close to 1 (and thus the remaining components are close to zero), then the prediction $\hat{\boldsymbol{p}}$ should be close to the prediction of Hinton's oracle which is provided by $\hat{\boldsymbol{T}}_i$.

\subsection{Normal coupling}


\subsection{Properties of coupling methods}


The following table indicates which properties are enjoyed by each coupling method we described in this section.

\begin{table}[!ht]
\begin{tabular}{cm{2.5cm}cccm{1.5cm}m{1.5cm}}
&property & Wu-Lin-Weng & radial & normal & arboreal & Hinton's oracle \\
\hline 
\multirow{4}{*}{\begin{turn}{90}\makecell{exact}\end{turn}}
&uniqueness & yes & yes & yes & yes & yes \\
&Bradley-Terry consistency & yes & yes & yes & yes & yes \\
&canonicity & no & yes & yes & no & yes \\
&symmetry & yes & yes & yes & no & yes \\
\hline
\multirow{5}{*}{\begin{turn}{90}\makecell{non-exact}\end{turn}}
&Hinton's minimality & ?  & ++ & ?  & ? & +++ \\
&lack of parameters & yes & yes & yes & yes & yes \\
& amounts to a linear system & yes & yes & yes & yes & yes\\
& linear system of special form & no & yes & yes & yes & yes \\
& iterative method & yes & yes & no & not needed & not needed\\
\hline
\end{tabular}
\caption{Summary of properties of coupling methods described in Section \ref{sec:coupling}.}
\label{tab:summaryCoupling}
\end{table}


%\subsection{Wu-Lin-Weng's method}



%\subsection{Linear kernel}

%\begin{tabular}
%\input{glm1-multi.tex}
%\end{tabular}

Here is a citation \cite{chow:68}.

% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

\acks{All acknowledgements go at the end of the paper before appendices and references.
Moreover, you are required to declare funding (financial activities supporting the
submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section{}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\section{}

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in 
\bibliography{paper}

\end{document}
