\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{jmlr2e}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{tikz-cd}

\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{assumption}{Assumption}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\DeclareMathOperator*{\divm}{div}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{23}{2025}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{Probabilistic OVO}{Probabilistic OVO}
\firstpageno{1}

\begin{document}

\title{On probabilistic one-vs-one classification}

\author{\name Ondrej Šuch \email ondrejs@savbb.sk \\
       \addr Matematický ústav SAV\\
       Ďumbierska 1\\
       Banská Bystrica, 974 01, Slovakia
       \AND
       \name Peter Novotný \email peter.novotny@uniza.sk \\
       \addr Fakulta informatiky a riadenia\\
       Žilinská Univerzita v Žiline\\
       Žilina, 010 26, Slovakia
       \AND
       \name Ali Haidar \email haidar@savbb.sk \\
       \addr Matematický ústav SAV\\
       Ďumbierska 1\\
       Banská Bystrica, 974 01, Slovakia
       }

\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\blindtext
\end{abstract}

\begin{keywords}
  keyword one, keyword two, keyword three
\end{keywords}

\section{Introduction}

%\blindmathpaper

Multi-class classification is considered to be a more challenging problem than binary classification.  A natural approach to multi-class classification is to reduce it to a series of binary classification problems and deduce the multi-class decision by aggregating decisions for the binary problems. 

One popular method belonging to  this paradigm uses probabilistic modelling for one-vs-one series of binary problems. Notably, it is used as the basis for multi-class classification models using support vector machines (SVM) in LIBSVM library. However one can use the approach more generally,  because it can be applied to any probabilistic binary classification method. 

A key step in probabilistic one-vs-one modelling is the aggregation of results of individual binary classifiers - so-called \emph{coupling method}. The most commonly used coupling method is that proposed by Wu-Lin-Weng. As noted in \cite{dohau}, the method is one of several non-canonical decisions used in multi-class SVM modelling. The goal of this paper is to compare its behavior with canonical methods. 


\section{Theoretical basis of coupling methods}

Suppose we have $K$ classes of objects, each distributed according to a probability distribution $p_i$, $i=1,\ldots, K$ on a feature space $X$. In a multi-class classification problem we are provided with the prior probabilities $\Pi = (P_1, \ldots, P_k)$ of each class. Given $x$ in $X$ we aim to find the probability that $x$ belongs to $i$-th class. 


\subsection{Bayes classifier}

An exact solution to this problem is provided by the Bayes classifier. To simplify the discussion we will make the following assumption.


\begin{assumption} \label{ass:1}
 For all $x$ in the feature space we have $p_i(x) > 0$.
\end{assumption}
Commonly used classifiers such as linear discriminant analysis, (penalized) logistic regression or support vector machines all satisfy this assumption.

The Bayes classifier predicts that the probability of $x$ belonging to the $i$-th class is
\begin{align}
 p^\Pi_i(x) = \frac{p_i(x) P_i}{\sum_{k=1}^K p_k(x)P_k}.
\end{align}

Besides the multi-class classifier we may also consider the binary Bayes classifiers for any pair $(i,j)$ of classes. Assuming prior on the two classes is proportional to $(P_i, P_j)$, the binary Bayes classifier predicts the probability of $i$-th class as 

\begin{align}
	p_{ij}^\Pi(x) = \frac{p_i(x) P_i}{p_i(x)P_i + p_j(x)P_j}.
\end{align}

Knowledge of vector $\boldsymbol{p}^\Pi(x)$ is  equivalent to knowing the functions $p^\Pi_{ij}(x)$. Indeed under Assumption \ref{ass:1} we have
\begin{align}
p_{ij}^\Pi(x) = \frac{p_i^\Pi(x)}{p_i^\Pi(x) + p_j^\Pi(x)}. \label{eq:bt1}
\end{align}

Conversely, it is easy to show that  the system of equations \eqref{eq:bt1} has a unique solution.

\begin{prop} \label{prop:binary2multi}
The system of  equations \eqref{eq:bt1} in unknowns $p_i^\Pi(x)$ has a unique solution which is the multi-class Bayes classifier.
\end{prop}
%\begin{proof}
%Indeed, we have 
%\begin{align*}
%\frac{1}{r_{ij}(x)} - 1 = \frac{p_j^\Pi}{p_i^\Pi}
%\end{align*}
%\end{proof}


\subsection{One-vs-one classification }


In practice one does not have know true probability distributions and hence cannot construct Bayes classifier. However, given a sample from the distribution, we can train binary classifiers to provide approximations $r_{ij}(x)$ to pairwise Bayes classifiers $p_{ij}^\Pi(x)$. The goal of probabilistic one-vs-one classification is to deduce for any $x$ in the feature space $X$ an estimate $\hat{\boldsymbol{p}}(x)$ for $\boldsymbol{p}^\Pi(x)$. One does it by finding an approximate solution of the system of equations
%\begin{align}
%
%\end{align}
%
%
%Suppose that $K$ classes $C_1, \ldots, C_K$ are distributed according to probability distributions $p_i$ on a  space $X$. Let us denote the $r_{ij}(x)$ the output of Bayes (binary) classifier for $x$ in $X$ yielding the probability $p(x \in C_i \mid x\in C_i \cup C_j )$. Then we have
%\begin{equation}
%r_{ij}(x)= \frac {p_i(x)}{p_i(x) + p_j(x)}.
%\end{equation}
%
%
%The system of equations is called Bradley--Terry equations.  The equations in \eqref{eq:bt1}  can be transformed to a system of linear equations. The theoretical basis for one-vs-one classification framework is provided by the following result.
%
%
%
%Suppose we have estimators $\hat r_{ij}$ for Bayes predictions $r_{ij}$ for all pairs $i\not= j$. A \emph{coupling method} obtains a multi-class probabilistic estimate $\boldsymbol{\hat p}= (\hat p_1, \ldots, \hat p_K)$ that satisfy
\begin{equation}
	{r}_{ij}(x) \approx \frac {\hat p_i(x)}{\hat p_i(x) + \hat p_j(x)}. \label{eq:bt2}
\end{equation}

% One should note that 
%\begin{itemize}
%\item the resulting system of equations will usually not be consistent, because $\hat{r}_{ij}$ are only estimates of true values $r_{ij}$,
%\item when probabilities are parametrized in other ways (e.g. other common parametrizations are as odds, or log-odds), the system of equations \eqref{eq:bt} is non-linear.
%\end{itemize}

Equations forming system \eqref{eq:bt2} are called Bradley-Terry equations.
The system is usually inconsistent and additional assumptions are needed. A method to solve Bradley-Terry equations is called a coupling method. Its input can be represented by a matrix $\boldsymbol{R}(x)$ with off-diagonal entries being $r_{ij}(x)$. Schematically, classification using coupling method $\boldsymbol{C}$ proceeds as indicated in Figure \ref{fig:coupling}.

\begin{figure}[!h]
	\centering
	\begin{tikzcd}[column sep=3cm]
		\textrm{sample $x$}\arrow[r, "binary~classifiers"] \arrow{dr}[swap]{multi-class~ prediction} & \boldsymbol{R}(x) \arrow{d}{coupling}[swap]{V} \\
		 & V(\boldsymbol{R}(x)) 
	\end{tikzcd}
	\caption{Multi-class classification using a coupling method ${V}$}
	\label{fig:coupling}
\end{figure}





\subsection{Exact desiderata on coupling method} \label{sec:des:exact}

Let us outline what kind of conditions may be desirable in a coupling method. In %Section \ref{sec:des:exact} 
this section
 we will describe exact mathematical criteria, while in section \ref{sec:des:inexact} we shall describe more flexible properties.

%\begin{itemize}
%\item a coupling method should should always provide a unique solution, at least when all $r_{ij}>0$,
%\item Bradley-Terry consistency,
%\item canonicity,
%\item symmetry,
%\item Bayes covariance,
%
%\item good accuracy on benchmark tasks,
%\item lack of parameters, or at least having a small number of parameters,
%\item Hinton's minimality,
%\item simplicity of computation.
%\end{itemize}



\emph{Unique solution} refers to the fact that the procedure to compute a multi-class probabilistic prediction always converges to a unique solution. One way to satisfy this condition would be that it amounts to a linear system of equations in $K$ variables, and the matrix of the system would be invertible. Another example would be when the procedure amounts to finding the minimum of a strongly convex function. 

\emph{Bradley-Terry consistency} refers to the outcome of a procedure providing a unique solution. The requirement states that should we apply the coupling procedure to binary Bayes classifiers, then the result is the output of multi-class Bayes classifier i.e. satisfies \eqref{eq:bt1}.

\emph{Canonicity} refers to requirement that the method should be in a natural way unique. Borrowing an example from regression analysis, Gauss-Markov theorem states that ordinary least squares (OLS) estimates is best linear unbiased estimate of the coefficients of a linear model. Therefore OLS  is a \emph{canonical} way to estimate parameters of a regression model. Note that potentially there may be multiple canonical methods, derived from (or satisfying) differing sets of assumptions. 

\emph{Symmetry} refers to the action of the permutation group which acts both on the classes as well as on multi-class predictions. The action of a permutation $\sigma$ extends to the action on the pairwise matrix $\boldsymbol{R} = (r_{ij})$ by 
\begin{equation}
	 r^\sigma_{i,j} \stackrel{def}{=}  r_{\sigma(i), \sigma(j)}.
\end{equation}
We say that a coupling method $V$ is \emph{symmetric} if 
\begin{align}
		V(\boldsymbol{R})^\sigma = V(\boldsymbol{R}^\sigma)\quad\textrm{for any permutation $\sigma$}.
\end{align}

\emph{Bayes covariance} is a notion introduced in  \cite{vsuch2016bayes}. It refers to the behavior of a probabilistic multi-class method built in one-vs-one fashion, when the priors change. Any Bayes classifier, whether binary or multiclass changes its prediction when the priors change. Let us describe the process. Suppose we change priors from $\Pi = (P_1, \ldots, P_k)$ to $\Pi'= (P'_1, \ldots, P'_K)$. Then the prediction of Bayes classifiers changes from $\boldsymbol{p}^\Pi= (p_1, \ldots, p_K)$ to the vector 
\begin{align}
\boldsymbol{p}^{\Pi'} \propto (p_1 \frac{P'_1}{P_1}, \ldots, p_K \frac{P'_K}{P_K}). \label{eq:changePrior}
\end{align}

When we use a coupling method and the prior changes, we  can apply \eqref{eq:changePrior} either before or after coupling. If the results are identical, for all initial pairwise data $\boldsymbol{R}$, then we say that the coupling method is Bayes covariant. Symbolically, let us denote by  $\pi$ the change of priors. Let $\boldsymbol{R}^\pi$ be pairwise datum when we apply \eqref{eq:changePrior} to each binary classifier.
For Bayes covariant method $V$ the following diagram commutes.

\begin{figure}[!h]
\centering
\begin{tikzcd}
	\boldsymbol{R} \arrow[r, "\pi"] \arrow[d, "V"] & \boldsymbol{R}^\pi \arrow[d, "V"] \\
	V(\boldsymbol{R}) \arrow[r, "\pi"] & V(\boldsymbol{R}^\pi) = V(\boldsymbol{R})^\pi
\end{tikzcd}
\caption{Diagrammatical description of Bayes covariance for a coupling method $V$}
\label{fig:bc}
\end{figure}


\subsection{Inexact desiderata for coupling methods}

\label{sec:des:inexact}

In this section we describe several concepts that lack strict mathematical delineation, but may still be important to consider.

\emph{Good accuracy on benchmark tasks} is  a strong argument to prefer a particular coupling method. This criterion is of course subjective, because there is a great variety of datasets in practice and inevitably there will be instances where any coupling method would underperform. However a good coupling method should perform robustly across a variety of datasets. A historical  example to follow is linear discriminant (LDA) of  Sir Ronald Fisher. At the time he was submitting his paper describing LDA, he had confirmed usefulness of his method not only on the famous iris dataset, but also on two separate collaborations with archaeologists. 

\emph{Lack of parameters} is strongly beneficial for situations when the dataset is small and it would be imprudent to reserve a sizeable portion to obtain unbiased estimates of the parameters of the coupling method. This issue gains expediency when the underlying binary classification method requires crossvalidation to select some hyperparameters. For example, this is the case of support vector machines, where often one opts for RBF kernel, which requires choosing two hyper-parameters. 

\emph{Simplicity of computation}  may refer to the computational requirement for inference. With the dramatic increase of edge computing devices, such as smartphones, a simple, fast, energy-efficient algorithm may be preferable to a better performing one which would require more computational resources. 

Let us finally mention \emph{Hinton's minimality}. Geoffrey Hinton is reported to object to the underlying principle of one-vs-one classification, where any single classifier influences the multi-class prediction \cite[p.~467]{hastie1998classification}. Suppose we are classifying objects from CIFAR-10 dataset.  If the true class is  a ``dog'', why should we care what is the output of the classifier distinguishing ``airplanes'' from ``ships''? The binary classifier is trained only on airplanes and ships, and has never seen a dog. One may thus prefer a classifier which somehow suppresses noisy output of (seemingly) irrelevant classifiers.


%The first that the five conditions have a clear mathematical definition, whereas the rest are not exact. For instance while the ideal situation for a coupling method is to have no parameters, the presence of one or two trainable parameters may be justified, if it yields noticeably better performance.


\section{Examples of coupling methods} \label{sec:coupling}

There is a plethora of methods available for coupling. In this section we describe four major classes.

\subsection{Methods aiming to minimize binary divergence}

A multi-class estimate $\hat{\boldsymbol{p}}$ obtained by a coupling method implies via \eqref{eq:bt1} probability distributions on each pair of classes:
\begin{align*}
	\hat D_{ij}^\textrm{multi}= \biggl(\frac{\hat{p_i}}{\hat p_i + \hat p_j},\frac{\hat{p_j}}{\hat p_i + \hat p_j}\biggr)	
\end{align*}
On the other hand, binary classifiers provide another distribution for each pair of  classes, namely
\begin{align*}
\hat D_{ij}^\textrm{binary} = (\hat r_{ij}, \hat r_{ji}).
\end{align*}

A natural way to find  $\hat{\boldsymbol{p}}$ is thus to minimize a functional incorporating divergence measures between these distributions
\begin{align*}
\hat{\boldsymbol{p}} = \argmin_{\boldsymbol{p}} \sum_{ij} w_{ij} \divm (\hat D_{ij}^\textrm{multi}, \hat D_{ij}^\textrm{binary})
\end{align*}

A notable example which uses this principle is the coupling method introduced in \cite{hastie1998classification}. Their method uses Kullback-Leibler divergence $\divm_\textrm{KL}$
\begin{align*}
	\divm\nolimits_\textrm{KL} (\boldsymbol{p}, \boldsymbol{q})= \sum p_i \log (p_i/ q_i)
\end{align*}

Kullback-Leibler divergence strongly penalizes  mispredictions when the true class is predicted to have very low probability. In the context of classification, when we are primarily concerned with accuracy , one could advantageously use other Bregman divergence derived from a proper score [\cite{gneiting2007strictly, buja2005loss}].
 
For instance, a popular alternative is divergence arising from the quadratic  score [\cite{gneiting2007strictly}] (also known as Brier score [\cite{brier1950verification}]). This divergence takes the form
$$
\divm\nolimits_\textrm{quad} (\boldsymbol{p}, \boldsymbol{q})= \sum (p_i - q_i)^2.
$$
Unlike Kullback-Leibler divergence, the penalty for incorrect prediction with arbitrarily low probability is bounded (by one). This quadratic score is the basis of the popular Wu-Lin-Weng's method [\cite{wu2004probability}]. 

Their method uses an additional trick, which we term \emph{self-referentiality}. In the method of Hastie and Tibshirani, $w_{ij}$ are constants derived from observed class frequencies. However, we could use weights $w_{ij}$ that increase when the predicted probability of either class $i$ or $j$ is high. A reasonable choice would be for instance $w_{ij}= p_i + p_j$. In the case of quadratic score, it is more convenient to use weights $w_{ij} = (p_i + p_j)^2$ because then the resulting optimization problem is just minimization of a quadratic form
\begin{align*}
\hat{\boldsymbol{p}} \stackrel{def}{=} \argmin_{\boldsymbol{p}} \sum_{i,j} (r_{ij}p_j - r_{ji}p_i)^2.
\end{align*}

We note that the method is symmetric. The choice of quadratic scoring rule can also be viewed as canonical in view of axiomatic characterization given in  [\cite{selten1998axiomatic}].  However the choice of weights $w_{ij}$ to make the method computationally amenable is somewhat arbitrary. Thus we do not view Wu-Lin-Weng's method as canonical.

\subsection{Arboreal methods}

System of equations \eqref{eq:bt2} is overdetermined because there are $\binom{K}{2}$ equations together with the requirement that the total sum of predicted probabilities is one.  Given an overdetermined system of equations, a common approach is to select a minimal subset of equations needed to solve for the unknowns. This principle underlies arboreal coupling methods.

An arboreal coupling method ignores all but $K-1$ of the pairwise predictions. A simple example for 3 classes is the classifier which ignores prediction of the binary classifier distinguishing between classes 1 and 3. The two equations form \eqref{eq:bt2} together with the requirement that the prediction probabilities should sum to 1 yield the following system of equations
\begin{equation}
	\begin{split}
		\frac{\hat p_1}{\hat p_1 + \hat p_2} &= {r}_{12}\\
		\frac{\hat p_2}{\hat p_2 + \hat p_3} &= {r}_{23}\\
		\hat p_1 + \hat p_2 + \hat p_3 &= 1
	\end{split}
	\label{eq:arb1}
\end{equation}

For $K=3$, there are three different arboreal coupling methods, which we shall call $T_1$, $T_2$, $T_3$, where the system \eqref{eq:arb1} represents coupling method $T_2$.

In order to obtain a regular system of equations for  larger values of $K$ , the resulting classifiers must represent a connected tree within the complete graph on all classes. Any arboreal method is non-canonical, because there is no natural way to choose a tree in a complete graph.

\subsection{Methods emphasizing Hinton's minimality}

A variant on arboreal methods is the following oracle method which we shall call Hinton's oracle. It  assumes it has the access to the ground truth $y = C_t$. It outputs probabilities by solving only the equations
$$
\frac{\hat p_t}{\hat p_t + \hat p_j} = {r}_{tj},\quad \textrm{for }j\not = t,
$$
together the  with total sum equation
$$
\sum_{i=1}^K  \hat p_i = 1.
$$

For any sample, its result is identical to the arboreal method where the underlying tree forms a star. This method is clearly the optimal one with respect to Hinton's minimality criterion, because it ignores the output of any classifier not trained on the true class. It is not a true classification method, however, because it ``peeks'' at the label. Its main utility is to provide a way to benchmark other methods with respect to Hinton's minimality.

In order to convert Hinton's oracle into a true classification method we can use the principle of self-referentiality.  Denote by ${T}_i$ the prediction of the arboreal method, where the underlying tree is a star centered at class $C_i$. The predictions will generally span the vector space $R^K$. In that case the prediction $\hat{\boldsymbol{p}}$ of the method  can be expressed as a linear combination of $T_i(x)$. Let us now define \emph{radial} coupling method. The  method requires that for the  weights we could in fact use the components of $\boldsymbol{p}$ i.e.
\begin{equation}
	\begin{split}
	\hat {\boldsymbol{p}} &= \hat p_1 T_1(x) + \ldots + \hat p_K T_K(x)\\
	1 &= \hat p_1 + \ldots + \hat p_K
	\end{split}
	 \label{eq:radial}
\end{equation}
%
The intuition behind \eqref{eq:radial} is that if some component of $\hat{\boldsymbol{p}}$ is close to 1 (and thus the remaining components are close to zero), then the prediction $\hat{\boldsymbol{p}}$ should be close to the prediction of Hinton's oracle which is provided by ${{T}}_i$.

\subsection{Bayes covariant methods}

There are many Bayes covariant methods. For instance any arboreal method is Bayes covariant. 



Given several Bayes covariant methods one can form  their ``linear combination'' - an ensemble which is also Bayes covariant. Let us make it explicit.

 Suppose real numbers $\alpha_1, \alpha_L$ are real numbers and $\boldsymbol{C}_1, \ldots, \boldsymbol{C}_L$ are coupling methods. We define their linear combination by requiring that 
 \begin{align*}
 	\bigl(\alpha_1 \boldsymbol{C}_1 \oplus \ldots \oplus \alpha_L \boldsymbol{C}_L\bigr)(x) \propto 
 \end{align*}
 
 given such $\alpha + \beta = 1$. If the prediction of the two Bayes covariant methods $\boldsymbol{B}$ and $\boldsymbol{\tilde B}$ are
\begin{align*}
	\boldsymbol{B}(x) = (p_1, \ldots, p_K)\quad\textrm{and}\quad \boldsymbol{\tilde B}(x) = (\tilde p_1, \ldots, \tilde p_K)
\end{align*}
then the prediction of the ensemble $\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}$ is given by
\begin{align*}
	\bigl(\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}\bigr)(x)\propto (\,p_1^\alpha \tilde p_1^\beta, \ldots, p_K^\alpha \tilde p_K^\beta).
\end{align*}

As an example, consider the classification problem with three classes. We have three arboreal classifiers $\boldsymbol{T}_1, \boldsymbol{T}_2, \boldsymbol{T}_3$ which correspond to star graphs centered respectively at classes $C_1, C_2, C_3$. The classifier 
\begin{align}
\frac13 \boldsymbol{T}_1 \oplus \frac13 \boldsymbol{T}_2 \oplus \frac 13 \boldsymbol{T}_3 = \frac13 \boldsymbol{T}_1 \oplus \frac23 \biggl(\frac12 \boldsymbol{T}_2 \oplus \frac 12 \boldsymbol{T}_3
\biggr), \label{eq:bc1}
\end{align}
is Bayes covariant \emph{and} symmetric. A key result proved in \cite{vsuch2016bayes} is that it is a unique such classifier for $K=3$.

\begin{thm} \label{thm:K3}
	There exists a unique symmetric Bayes covariant coupling methods for $K=3$.
\end{thm}

Let us point out one consequence of the theorem. We could define another ensemble $\boldsymbol{E}$ by averaging the predictions of $\boldsymbol{T}_1, \boldsymbol{T}_2, \boldsymbol{T}_3$:
\[
\boldsymbol{E} = \frac13 \boldsymbol{T}_1(x) + \frac13 \boldsymbol{T}_2(x) + \frac 13 \boldsymbol{T}_3(x).
\]
Note that here we use the plus symbol to denote the vector addition of vectors representing $K$-class probability distributions. This is a different classifier from the one given by  \eqref{eq:bc1} (see Appendix \ref{app:explicit}), and since it is symmetric, it cannot be Bayes covariant.

\subsection{Properties of coupling methods}


The following table indicates which properties are enjoyed by each coupling method we described in this section.

\begin{table}[!ht]
\begin{tabular}{cm{2.5cm}m{1.5cm}m{1.5cm}ccm{1.5cm}m{1.5cm}}
&property & Hastie-Tibshirani & Wu-Lin-Weng & radial & normal & arboreal & Hinton's oracle \\
\hline 
\multirow{5}{*}{\begin{turn}{90}\makecell{exact}\end{turn}}
&uniqueness &  yes & yes & yes & yes & yes & yes \\
&Bradley-Terry consistency & yes & yes & yes & yes & yes & yes \\
&canonicity & yes & no & yes & yes & no & yes \\
&symmetry & yes & yes & yes & yes & no & yes \\
& Bayes covariant & no & no & no & yes & yes & yes \\
\hline
\multirow{5}{*}{\begin{turn}{90}\makecell{non-exact}\end{turn}}
&Hinton's minimality & ? & ?  & ++ & ?  & ? & +++ \\
&lack of parameters & yes & yes & yes & yes & yes & yes \\
& amounts to a linear system & no & yes & yes & yes & yes & yes\\
& linear system of special form & no & no & yes & yes & yes & yes \\
& iterative method & yes & yes & yes & no & not needed & not needed\\
\hline
\end{tabular}
\caption{Summary of properties of coupling methods described in Section \ref{sec:coupling}.}
\label{tab:summaryCoupling}
\end{table}

\section{Experimental results.}

It is natural to consider the following interrelated questions 

\begin{itemize}
\item[a)] Which non-parametric coupling methods shows good performance across diverse data sets? Is it the orthodox method of Wu-Lin-Weng?
\item[b)] If the answer to a) is a coupling method which is not Bayes covariant, can we find a parametric method Bayes covariant method which  matches its performance?
\item[c)] If the answer to b) is negative, how serious is the impact of the lack of Bayes covariance in practice?
\end{itemize}

The following sections attempt to gather answers to these questions. 

\subsection{Datasets}

It is natural to consider the benchmark datasets used in previous evaluation of coupling methods \cite{wu2004probability}. The authors considered altogether seven datasets with number of classes ranging from 3 to 26. For each dataset they extracted 40 pairs of training and testing datasets. Half of these were smaller (300 training samples and 500 testing samples) and the other half was larger (800 training samples and 1000 testing samples). In our experiments we use only the larger pairs. 

As a preliminary step we conducted analysis of linear separability of the datasets. We used ECOS solver in CVXR library to solve the underlying quadratic program. The results are reported in Table \ref{tab:sep}.

\input{tab-sep.tex}

We can infer that except for 'waveform' dataset, majority of binary problems are linearly separable.

\subsection{Binary classifiers}

We have opted to use LDA as a binary classification tool. Its appeal is that it is applicable even for linearly separable datasets, thus obviating additional crossvalidation step incurred by methods that use regularization (e.g. penalized logistic regression or SVM). It usually provides results close to logistic regression \cite{james2013introduction}.

For preprocessing we first removed features that were constant across classes. Then we projected the data using PCA to the subspace for which corresponding singular values were at least $1/1000$. We used PCA without scaling or centering since the datasets' features were already normalized to interval $[-1,1]$. 

For training LDA we used MASS library in R.

%\begin{figure}[!ht]
%	\includegraphics[width = 0.5\linewidth]{graph/dna-lda.pdf}
%	\includegraphics[width = 0.5\linewidth]{graph/waveform-lda.pdf}
%	\caption{Projections of three-class datasets using LDA Left: dna, right: waveform}
%\end{figure}

\subsection{Experiment 1: Accuracy of parameter-less coupling methods on benchmark datasets}

The first experiment compares the performanc of three parameterless coupling methods
\begin{itemize}
\item method of Wu-Lin-Weng, which is widely used in software libraries, and thus can be viewed as an orthodox choice,
\item normal coupling, which is a Bayes covariant method,
\item radial coupling, which is not Bayes covariant.
\end{itemize}

Although previous evaluations of the latter two methods were done in \cite{vsuch2016bayes}, they used phonetic data for which ground truth may be imprecise. 

\input{tab-multi.tex}

From Table \ref{tab:multi} we can clearly see that Wu-Lin-Weng's method has the best performance. On some datasets it trounces normal coupling method by a wide margin. The edge over radial method is smaller, with the exception of usps dataset where its accuracy is higher by remarkable 30\%. 



\subsection{Experiment 2: Accuracy of Bayes covariant methods with parameters}

In the previous experiment we established that the method of Wu-Lin-Weng provides leading performance. However in view of Theorem \ref{thm:K3} the method is not Bayes covariant. 

In this experiment we investigated whether we can (always) find  a Bayes covariant coupling method that could match the performance of Wu-Lin-Weng's method, if we allow it to have (a few) trainable parameters.  Our experiment proceeds in five steps. 

First, we restrict our investigation to the case of three class datasets. This is a natural starting point, since it is the smallest number of classes where the question is meaningful to ask. 

As a second step, we consider whether the three arboreal methods $ \boldsymbol{S}_1,~ \boldsymbol{S}_2,~ \boldsymbol{S}_3$ (which do not have any parameters) could bridge the gap. The statistics are shown in Table \ref{tab:step2}. 

\input{tab-step2.tex}


We can see that approximately half of the cases none of the four parameterless Bayes covariant methods was able to match the performance of Wu-Lin-Weng's method. Therefore we should extend search to Bayes covariant methods with trainable parameters.


In the third step we define two parameter family of Bayes covariant coupling methods
\begin{align}
a_1 \boldsymbol{S}_1 + a_2 \boldsymbol{S}_2 + a_3 \boldsymbol{S}_3,\quad\textrm{where } a_1 + a_2 +a_3 = 1
\end{align}
Note that the parameter space of this family is not compact. In order to facilitate computations we opted to conduct a grid search on points in the simplex where the coefficients $a_i$ are nonnegative $a_i \geq 0$. 

In order to reduce computational burden we selected 1000 three-class subproblems. The results are shown in Figure \ref{fig:par-bc}.

\begin{figure}[!ht]
\includegraphics{graph/exp2-summary.pdf}
\caption{Results of grid search for parametric Bayes covariant methods matching performance of Wu-Lin-Weng's method}
\label{fig:par-bc}
\end{figure}




\begin{figure}[!ht]
	\includegraphics{graph/exp2-detail.pdf}
	\caption{Color map indicates the value of Brier score computed for an ensemble of coupling methods on the testing dataset. The vertices triangle indicate the arboreal coupling methods, and the black circle the location of the optimum.}
\end{figure}

%\subsection{Wu-Lin-Weng's method}



%\subsection{Linear kernel}

%\begin{tabular}
%\input{glm1-multi.tex}
%\end{tabular}

%Here is a citation \cite{chow:68}.

% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

\acks{All acknowledgements go at the end of the paper before appendices and references.
Moreover, you are required to declare funding (financial activities supporting the
submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section{}
\label{app:explicit}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we provide explicit predictions of coupling methods used in the paper.



In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\section{}

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in 
\bibliography{paper}

\end{document}
