

\section{Examples of coupling methods} \label{sec:coupling}

There is a plethora of methods available for coupling. In this section we describe four major classes.

\subsection{Methods aiming to minimize binary divergence}

A multi-class estimate $\hat{\boldsymbol{p}}$ obtained by a coupling method implies via \eqref{eq:bt1} probability distributions on each pair of classes:
\begin{align*}
	\hat D_{ij}^\textrm{multi}= \biggl(\frac{\hat{p_i}}{\hat p_i + \hat p_j},\frac{\hat{p_j}}{\hat p_i + \hat p_j}\biggr)	
\end{align*}
On the other hand, binary classifiers provide another distribution for each pair of  classes, namely
\begin{align*}
\hat D_{ij}^\textrm{binary} = (\hat r_{ij}, \hat r_{ji}).
\end{align*}

A natural way to find  $\hat{\boldsymbol{p}}$ is thus to minimize a functional incorporating divergence measures between these distributions
\begin{align*}
\hat{\boldsymbol{p}} \stackrel{def}{=} \argmin_{\boldsymbol{p}} \sum_{ij} w_{ij} \divm (\hat D_{ij}^\textrm{multi}, \hat D_{ij}^\textrm{binary})
\end{align*}

A notable example which uses this principle is the coupling method introduced in [\cite{hastie1998classification}]. Their method uses Kullback-Leibler divergence $\divm_\textrm{KL}$
\begin{align*}
	\divm\nolimits_\textrm{KL} (\boldsymbol{p}, \boldsymbol{q})= \sum p_i \log (p_i/ q_i)
\end{align*}

Kullback-Leibler divergence strongly penalizes  mispredictions when the true class is predicted to have very low probability. In the context of classification, when we are primarily concerned with accuracy , one could advantageously use other Bregman divergence derived from a proper score [\cite{gneiting2007strictly, buja2005loss}].
 
For instance, a popular alternative is divergence arising from the quadratic  score [\cite{gneiting2007strictly}] (also known as Brier score [\cite{brier1950verification}]). This divergence takes the form
$$
\divm\nolimits_\textrm{quad} (\boldsymbol{p}, \boldsymbol{q})= \sum (p_i - q_i)^2.
$$
Unlike Kullback-Leibler divergence, the penalty for incorrect prediction with arbitrarily low probability is bounded (by one). This quadratic score is the basis of the popular Wu-Lin-Weng's method [\cite{wu2004probability}]. 

Their method uses an additional trick, which we term \emph{self-referentiality}. In the method of Hastie and Tibshirani, $w_{ij}$ are constants derived from observed class frequencies. However, we could use weights $w_{ij}$ that increase when the predicted probability of either class $i$ or $j$ is high. A reasonable choice would be for instance $w_{ij}= p_i + p_j$. In the case of quadratic score, it is more convenient to use weights $w_{ij} = (p_i + p_j)^2$ because then the resulting optimization problem is just minimization of a quadratic form
\begin{align*}
\hat{\boldsymbol{p}} \stackrel{def}{=} \argmin_{\boldsymbol{p}} \sum_{i,j} (r_{ij}p_j - r_{ji}p_i)^2.
\end{align*}

We note that the method is symmetric. The choice of quadratic scoring rule can also be viewed as canonical in view of axiomatic characterization given in  [\cite{selten1998axiomatic}].  However the choice of weights $w_{ij}$ to make the method computationally amenable is somewhat arbitrary. Thus we do not view Wu-Lin-Weng's method as canonical.

\subsection{Arboreal methods}

System of equations \eqref{eq:bt2} is overdetermined because there are $\binom{K}{2}$ equations together with the requirement that the total sum of predicted probabilities is one.  Given an overdetermined system of equations, a common approach is to select a minimal subset of equations needed to solve for the unknowns. This principle underlies arboreal coupling methods.

An arboreal coupling method ignores all but $K-1$ of the pairwise predictions. A simple example for 3 classes is the classifier which ignores prediction of the binary classifier distinguishing between classes 1 and 3. The two equations form \eqref{eq:bt2} together with the requirement that the prediction probabilities should sum to 1 yield the following system of equations
\begin{equation}
	\begin{split}
		\frac{\hat p_1}{\hat p_1 + \hat p_2} &= {r}_{12}\\
		\frac{\hat p_2}{\hat p_2 + \hat p_3} &= {r}_{23}\\
		\hat p_1 + \hat p_2 + \hat p_3 &= 1
	\end{split}
	\label{eq:arb1}
\end{equation}

For $K=3$, there are three different arboreal coupling methods, which we shall call $\boldsymbol{s}_1$, $\boldsymbol{s}_2$, $\boldsymbol{s}_3$, where the system \eqref{eq:arb1} represents the coupling method $\boldsymbol{s}_2$.

For $K>3$ the three methods can be generalized to corresponds to the trees having the graph structure  of a star centered at class $C_i$ for $i=1, \ldots, K$.

More generally, to obtain a regular system of equations for  any of $K$ , it is necessary that the tree corresponding to an arboreal method forms a spanning tree. Note that any arboreal method is non-canonical, because there is no natural way to choose a tree in a complete graph. In fact, a stronger statement is true -- no arboreal method is symmetric.
%However, we will see in Section \ref{sec:bc2} that by ``averaging'' several stars we can obtain a canonical 



\subsection{Methods emphasizing Hinton's minimality}

A variant on arboreal methods is the following oracle method which we shall call Hinton's oracle. It  assumes it has the access to the ground truth $y = C_t$. It outputs probabilities by solving only the equations
$$
\frac{\hat p_t}{\hat p_t + \hat p_j} = {r}_{tj},\quad \textrm{for }j\not = t,
$$
together the  with total sum equation
$$
\sum_{i=1}^K  \hat p_i = 1.
$$

For any one sample belonging to class $C_t$, its result is identical to the arboreal method where the underlying tree forms a star with center at $C_t$. This method is clearly the optimal one with respect to Hinton's minimality criterion, because it ignores the output of any classifier not trained on the true class. It is not a true classification method, however, because it ``peeks'' at the label. Its main utility is to provide a way to benchmark other methods with respect to Hinton's minimality.

In order to convert Hinton's oracle into a true classification method we can use the principle of self-referentiality.  Denote by $\boldsymbol{s}_i$ the prediction of the arboreal method, where the underlying tree is a star centred at the class $C_i$. The predictions will generally span the vector space $R^K$. In that case the prediction $\hat{\boldsymbol{p}}$ of the method  can be expressed as a linear combination of $\boldsymbol{s}_i(x)$. Let us now define \emph{radial} coupling method. The  method requires that for the  weights we could in fact use the components of $\boldsymbol{p}$ i.e.
\begin{equation}
	\begin{split}
	\hat {\boldsymbol{p}} &= \hat p_1 \boldsymbol{s}_1(x) + \ldots + \hat p_K \boldsymbol{s}_K(x)\\
	1 &= \hat p_1 + \ldots + \hat p_K
	\end{split}
	 \label{eq:radial}
\end{equation}
%
The intuition behind \eqref{eq:radial} is that if some component of $\hat{\boldsymbol{p}}$ is close to 1 (and thus the remaining components are close to zero), then the prediction $\hat{\boldsymbol{p}}$ should be close to the prediction of Hinton's oracle which is provided by ${\boldsymbol{s}}_i$.

\subsection{Bayes covariant methods} \label{sec:bc2}

There are many Bayes covariant methods. The simplest example are arboreal coupling methods. They are not canonical, but with the help of ensembling, one can construct also canonical coupling methods. 

A very helpful aid to study Bayes covariant methods is property $BC_{ij}$ (the notation stands for \emph{Bayes covariant when restricted to classes} $i$ and $j$). A coupling method $\boldsymbol{v}$ satisfies $BC_{ij}$ if for any change of priors $\pi$ and any  pairwise data $\boldsymbol{R}$ the odds of classes $C_i$ to $C_j$ predicted by $\boldsymbol{v}(\boldsymbol{R}^\pi)$ and $\bigl(\boldsymbol{v}(\boldsymbol{R})\bigr)^\pi$ are equal. Using earlier notation for odds (see \eqref{eq:alpha}), one has
\begin{align}
	\alpha_{ij} \bigl(\boldsymbol{v}(\boldsymbol{R}^\pi) \bigr) = \alpha_{ij} \bigl(\boldsymbol{v}(\boldsymbol{R})^\pi \bigr)
\end{align}

Let us list some easy to verify properties of this concept:

\begin{itemize}
	\item[a)] a Bayes covariant coupling method satisfies $BC_{ij}$ for any $i\not= j$ (see Proposition \ref{prop:bcprop} in Appendix \ref{app:bc1}),
	%\item[b)] if a coupling method satisfies $BC_{12}, BC_{23}, \ldots, BC_{(K-1),K}$ then it is Bayes covariant (see Proposition \ref{prop:bcprop}),
	\item[b)] if a coupling method satisfies $BC_{ij}$ for all $i\not= j$ then it is Bayes covariant (see Proposition \ref{prop:bcprop} in Appendix \ref{app:bc1}),
	\item[c)] if a coupling method satisfies $BC_{ij}$ and $BC_{jk}$ then it satisfies $BC_{ik}$ (see Lemma \ref{lem:transitivity} in Appendix \ref{app:bc1}).
\end{itemize}

Now we are ready to prove the our first result.

\begin{prop}
Any arboreal method is Bayes covariant.
\end{prop}
\begin{proof}

An arboreal coupling method $\boldsymbol{t}$ induced by a spanning tree $T$ on the classes satisfies $BC_{ij}$ whenever the edge $C_iC_j$ belongs to $T$. Since any pair of vertices is connected by a path, it follows from transitivity of $BC_{ij}$ (property c)\,) that the assumption of b) holds and thus the coupling method is Bayes covariant.
	
\end{proof}

Given several Bayes covariant methods one can form  their ``linear combination'' - an ensemble which is also Bayes covariant. Let us make the construction explicit.

Suppose first that real numbers $a_1, \ldots,a_M$ are real numbers and $\boldsymbol{v}_1, \ldots, \boldsymbol{v}_M$ are probabibilistic multi-class classification methods satisfying Assumption \ref{ass:2}. We define their linear combination $\bigoplus_i a_m \boldsymbol{v}_m$ as the classification method that for any $x$ yields the probability distribution on $K$ classes that satisfies
\begin{align*}
 	\bigl(a_1 \boldsymbol{v}_1 \oplus \ldots \oplus a_M \boldsymbol{v}_M\bigr)(x) \propto \boldsymbol{v}_1^{a_1}(x) \odot \cdots \odot \boldsymbol{v}_M^{a_M}(x),
\end{align*}
where the symbol $\odot$ denotes componentwise multiplication, and exponentials are computed component-wise. With this definition we can state the following result.

\begin{prop}
	\label{prop:ensemble}
Suppose the coupling methods $\boldsymbol{v}_1, \ldots, \boldsymbol{v}_M$ are Bayes covariant. If $\sum a_m = 1$, then $\boldsymbol{e} = \bigoplus_m a_m \boldsymbol{v}_m$ is also a Bayes covariant coupling method.
\end{prop}

\begin{proof}
It is sufficient to show that $BC_{ij}$ holds for any $i\not= j$. 

Let $\pi$ be a change of priors from $(P_1, \ldots, P_K)$  to $(P'_1, \ldots, P'_K)$. Let $o_m$ denotes the odds of classes $C_i$ and $C_j$ predicted by coupling method $\boldsymbol{v}_m$ i.e.
$$
o_m = \alpha_{ij}(\boldsymbol{v}_m(\boldsymbol{R})).
$$

It follows from the definition of linear combination of coupling methods that the odds of $C_i$ to $C_j$ for $\boldsymbol{e}$ are
\begin{align}
\prod_m o_m^{a_m}.
\end{align}
It follows that the odds of $\boldsymbol{e}^\pi$ are 
\begin{align}
\frac{P'_i}{P_i} \frac{P_j}{P'_j} \prod_m o_m^{a_m}.
\end{align}

Let us now use Bayes covariance of $\boldsymbol{v}_i$. 
We have
\begin{align}
	\biggl(\bigoplus_m a_m \boldsymbol{v}_m\biggr)(\boldsymbol{R^\pi}) & \propto 
	\boldsymbol{v}_1^{a_1}(\boldsymbol{R}^\pi) \odot \cdots \odot \boldsymbol{v}_M^{a_M}(\boldsymbol{R}^\pi) =  \bigodot_m  (\boldsymbol{v}_m^\pi(\boldsymbol{R}))^{a_m} \\
%	&= \frac{P'_i}{P_i} \frac{P_j}{P'_j} \prod_m o_m^{a_m}
\end{align}

It follows that the odds of $C_i$ to $C_j$ for $\bigoplus_m a_m \boldsymbol{v}_m$ are 
\begin{align}
\biggl(\frac{P'_i}{P_i} \frac{P_j}{P'_j} \biggr)^{\sum a_m} \prod_m o_m^{a_m}.
\end{align}
Clearly, under the assumption $\sum_m a_m = 1$, the classifier $\bigoplus_m a_m \boldsymbol{v}_m$ satisfies $BC_{ij}$. Since this holds for any $i\not=j$,  it follows that $\boldsymbol{e}$ is Bayes covariant.
\end{proof}


% 
% given such $\alpha + \beta = 1$. If the prediction of the two Bayes covariant methods $\boldsymbol{B}$ and $\boldsymbol{\tilde B}$ are
%\begin{align*}
%	\boldsymbol{B}(x) = (p_1, \ldots, p_K)\quad\textrm{and}\quad \boldsymbol{\tilde B}(x) = (\tilde p_1, \ldots, \tilde p_K)
%\end{align*}
%then the prediction of the ensemble $\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}$ is given by
%\begin{align*}
%	\bigl(\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}\bigr)(x)\propto (\,p_1^\alpha \tilde p_1^\beta, \ldots, p_K^\alpha \tilde p_K^\beta).
%\end{align*}

As an example, consider the classification problem with three classes. We have three arboreal classifiers $\boldsymbol{s}_1, \boldsymbol{s}_2, \boldsymbol{s}_3$ which correspond to star graphs centered respectively at classes $C_1, C_2, C_3$. The classifier 
\begin{align}
\boldsymbol{e}_3 \stackrel{def}{=} \frac13 \boldsymbol{s}_1 \oplus \frac13 \boldsymbol{s}_2 \oplus \frac 13 \boldsymbol{s}_3  \label{eq:bc1}
\end{align}
is Bayes covariant \emph{and} symmetric. A key result proved in [\cite{vsuch2016bayes}] is that it is a unique  classifier having these two properties.

\begin{thm} \label{thm:K3}
	There exists a unique symmetric Bayes covariant coupling methods for $K=3$.
\end{thm}

\begin{proof}
Let 
\begin{align}
	\boldsymbol{R}_s \stackrel{def}{=} \begin{pmatrix} \cdot & s & 1 -s \\  1-s & \cdot & s \\ s & 1-s & \cdot \end{pmatrix}
\end{align}
Since $R_s$ is invariant under the action of any permutation of 3 elements, 
for a symmetric coupling method we have 
\begin{align}
	\boldsymbol{v} (\boldsymbol{R}_s) = \biggl(\frac 13, \frac 13, \frac 13 \biggr).
\end{align}
Now applying  Proposition \ref{prop:bc3} from Appendix \ref{app:bc1} yields uniqueness. The existence of the method was already shown above using Proposition \ref{prop:ensemble}.
\end{proof}


Let us point out one consequence of the theorem. We could define another ensemble $\tilde{\boldsymbol{e}}$ by averaging the predictions of $\boldsymbol{s}_1, \boldsymbol{s}_2, \boldsymbol{s}_3$:
\[
\tilde{\boldsymbol{e}} = \frac13 \boldsymbol{s}_1(x) + \frac13 \boldsymbol{s}_2(x) + \frac 13 \boldsymbol{s}_3(x).
\]
Note that here we use the plus symbol to denote the vector addition of vectors representing  probability distributions over $K$ classes. This is a different classifier from the one given by  \eqref{eq:bc1} (see Appendix \ref{app:explicit}), and since it is symmetric, it cannot be Bayes covariant.

The classifier $\boldsymbol{e}_3$ has a canonical generalization for any $K>3$, namely the classifier
\begin{align}
\boldsymbol{e}_K \stackrel{def}{=} \frac1K \boldsymbol{s}_1 \oplus \frac1K \boldsymbol{s}_2 \oplus \ldots \oplus \frac 1K \boldsymbol{s}_K  \label{eq:bc1}
\end{align}
We call this coupling method the \emph{normal} coupling method.

\subsection{Properties of coupling methods}

In this section  we summarize properties of coupling methods described earlier in this section.  Table \ref{tab:summaryCoupling} concisely lists the relevant properties enjoyed by each method. Note that we have split the desideratum ``simplicity of computation'' into three different aspects:
\begin{itemize}
	\item whether a coupling method amounts to a linear system of equations,
	\item if the coupling method amounts to a linear system of equations, whether the equations are of a special form (e.g. a Markov matrix) that facilitates their solution,
	\item whether the authors who published the method provide an iterative method that implements the estimate. Iterative methods are often  faster than Gaussian elimination, and less prone to numerical issues.
\end{itemize}

 %interpretation of exact desiderate is unambiguous. 


%The following table indicates which properties are enjoyed by each coupling method we described in this section.

\begin{table}[!ht]
\begin{tabular}{cm{2.5cm}m{1.5cm}m{1.5cm}ccm{1.5cm}m{1.5cm}}
&property & Hastie-Tibshirani & Wu-Lin-Weng & radial & normal & arboreal & Hinton's oracle \\
\hline 
\multirow{5}{*}{\begin{turn}{90}\makecell{exact}\end{turn}}
&uniqueness &  yes & yes & yes & yes & yes & yes \\
&Bradley-Terry consistency & yes & yes & yes & yes & yes & yes \\
&canonicity & yes & no & yes & yes & no & yes \\
&symmetry & yes & yes & yes & yes & no & yes \\
& Bayes covariant & no & no & no & yes & yes & yes \\
\hline
\multirow{5}{*}{\begin{turn}{90}\makecell{non-exact}\end{turn}}
&Hinton's minimality & N/A & \multicolumn{5}{c}{see Section \ref{sec:exp1}} \\
&robustness & N/A & \multicolumn{5}{c}{see Section \ref{sec:exp1}} \\
&lack of parameters & yes & yes & yes & yes & yes & yes \\
& amounts to a linear system & no & yes & yes & yes & yes & yes\\
& linear system of special form & N/A & no & yes & yes & yes & yes \\
& iterative method & yes & yes & yes & no & not needed & not needed\\
\hline
\end{tabular}
\caption{Summary of properties of coupling methods described in Section \ref{sec:coupling}. The method of Hastie-Tibshirani is omitted from evaluation in Section \ref{sec:exp1}, because earlier evaluation showed it lacks robustness compared to the method of Wu-Lin-Weng [\cite{wu2004probability}].}
\label{tab:summaryCoupling}
\end{table}

