

\section{Examples of coupling methods} \label{sec:coupling}

There is a plethora of methods available for coupling. In this section we describe four major classes.

\subsection{Methods aiming to minimize binary divergence}

A multi-class estimate $\hat{\boldsymbol{p}}$ obtained by a coupling method implies via \eqref{eq:bt1} probability distributions on each pair of classes:
\begin{align*}
	\boldsymbol{d}_{ij}^\textrm{multi}= \biggl(\frac{\hat{p_i}}{\hat p_i + \hat p_j},\frac{\hat{p_j}}{\hat p_i + \hat p_j}\biggr)	
\end{align*}
On the other hand, binary classifiers provide another distribution for each pair of  classes, namely
\begin{align*}
\boldsymbol{d}_{ij}^\textrm{binary} = ( r_{ij},  r_{ji}).
\end{align*}

A natural way to find  $\hat{\boldsymbol{p}}$ is thus to minimize a functional incorporating divergence measures between these distributions
\begin{align*}
\hat{\boldsymbol{p}} \stackrel{def}{=} \argmin_{\boldsymbol{p}} \sum_{ij} w_{ij} \divm (\boldsymbol{d}_{ij}^\textrm{multi}, \boldsymbol{d}_{ij}^\textrm{binary})
\end{align*}

A notable example which uses this principle is the coupling method introduced in \cite{hastie1998classification}. Their method uses Kullback-Leibler divergence $\divm_\textrm{KL}$
\begin{align*}
	\divm\nolimits_\textrm{KL} (\boldsymbol{d}, \boldsymbol{d'})= \sum d_i \log (d_i/ d'_i)
\end{align*}

Kullback-Leibler divergence strongly penalizes  mispredictions when the true class is predicted to have very low probability. In the context of classification, when we are primarily concerned with accuracy , one could advantageously use other Bregman divergence derived from a proper score \cite{gneiting2007strictly, buja2005loss}.
 
For instance, a popular alternative is divergence arising from the quadratic  score \cite{gneiting2007strictly} (also known as Brier score \cite{brier1950verification}). This divergence takes the form
$$
\divm\nolimits_\textrm{quad} (\boldsymbol{d}, \boldsymbol{d'})= \sum (d_i - d'_i)^2.
$$

The quadratic score is the basis of the popular Wu-Lin-Weng's method \cite{wu2004probability}. 
Their method uses an additional trick, which we term \emph{self-referentiality}. In the method of Hastie and Tibshirani, $w_{ij}$ are constants derived from observed class frequencies. However, we could use weights $w_{ij}$ that increase when the predicted probability of either class $i$ or $j$ is high. A reasonable choice would be for instance $w_{ij}= p_i + p_j$. In the case of quadratic score, it is more convenient to use weights $w_{ij} = (p_i + p_j)^2$ because then the resulting optimization problem is just minimization of a quadratic form
\begin{align*}
\hat{\boldsymbol{p}} \stackrel{def}{=} \argmin_{\boldsymbol{p}} \sum_{i,j} (r_{ij}p_j - r_{ji}p_i)^2.
\end{align*}

We note that the method is symmetric. The choice of quadratic scoring rule can also be viewed as canonical in view of axiomatic characterization given in  \cite{selten1998axiomatic}.  However the choice of weights $w_{ij}$ to make the method computationally amenable is somewhat arbitrary. Thus we do not view Wu-Lin-Weng's method as canonical.

\subsection{Arboreal methods}

System of equations \eqref{eq:bt2} is overdetermined because there are $\binom{K}{2}$ equations together with the requirement that the total sum of predicted probabilities is one.  Given an overdetermined system of equations, a common approach is to select a minimal subset of equations needed to solve for the unknowns. This principle underlies arboreal coupling methods.

An arboreal coupling method ignores all but $K-1$ of the pairwise predictions. A simple example for 3 classes is the classifier which ignores prediction of the binary classifier distinguishing between classes $C_1$ and $C_3$. The two equations form \eqref{eq:bt2} together with the requirement that the prediction probabilities should sum to 1 yield the following system of equations
\begin{equation}
	\begin{split}
		\frac{\hat p_1}{\hat p_1 + \hat p_2} &= {r}_{12}\\
		\frac{\hat p_2}{\hat p_2 + \hat p_3} &= {r}_{23}\\
		\hat p_1 + \hat p_2 + \hat p_3 &= 1
	\end{split}
	\label{eq:arb1}
\end{equation}

For $K=3$, there are three different arboreal coupling methods corresponding to three different spanning trees on three vertices. We shall call them $\boldsymbol{s}_1$, $\boldsymbol{s}_2$, $\boldsymbol{s}_3$, where the coupling method $\boldsymbol{s}_i$ includes only the equations that involve class $i$. In particular, \eqref{eq:arb1} represents the coupling method $\boldsymbol{s}_2$.

For $K>3$ the three methods can be generalized to corresponds to the trees having the graph structure  of a star centered at class $C_i$ for $i=1, \ldots, K$.

More generally, to obtain a regular system of equations for  any of $K$ , it is necessary and sufficient that the tree formed by edges for which we include equations from Bradley-Terry system is  a spanning tree. Note that any arboreal method is non-canonical, because there is no natural way to choose a spanning tree in a complete graph. In fact, a stronger statement is true -- no arboreal method is symmetric. 
To see that, consider the action of the transposition of a vertex $v$ of degree one and another vertex $v'$ of degree $>1$ in a spanning tree. The results of coupling involves different number of equations for classes $C_v$ and $C_v'$ which is a contradiction.
%However, we will see in Section \ref{sec:bc2} that by ``averaging'' several stars we can obtain a canonical 


\subsection{Methods emphasizing Hinton's minimality}

A variant on arboreal methods is the following oracle method which we shall call Hinton's oracle. It  assumes it has the access to the ground truth $y = C_t$. It outputs probabilities by solving only the equations
$$
\frac{\hat p_t}{\hat p_t + \hat p_j} = {r}_{tj},\quad \textrm{for }j\not = t,
$$
together the  with total sum equation
$$
\sum_{i=1}^K  \hat p_i = 1.
$$

For any one sample belonging to class $C_t$, its result is identical to the arboreal method where the underlying tree forms a star with center at $C_t$. This method is clearly the optimal one with respect to Hinton's minimality criterion, because it ignores the output of any classifier not trained on the true class. It is not a true classification method, however, because it ``peeks'' at the label. Its main utility is to provide a way to benchmark other methods with respect to Hinton's minimality.

In order to convert Hinton's oracle into a true classification method we can use the principle of self-referentiality.  Denote by $\boldsymbol{s}_i$ the prediction of the arboreal method, where the underlying tree is a star centred at the class $C_i$. The predictions will generally span the vector space $R^K$. In that case the prediction $\hat{\boldsymbol{p}}$ of the method  can be expressed as a linear combination of $\boldsymbol{s}_i(x)$. Let us now define \emph{radial} coupling method. The  method requires that for the  weights we could in fact use the components of $\boldsymbol{p}$ i.e.
\begin{equation}
	\begin{split}
	\hat {\boldsymbol{p}} &= \hat p_1 \boldsymbol{s}_1(x) + \ldots + \hat p_K \boldsymbol{s}_K(x)\\
	1 &= \hat p_1 + \ldots + \hat p_K
	\end{split}
	 \label{eq:radial}
\end{equation}
%
The intuition behind \eqref{eq:radial} is that if some component $\hat p_i$ of $\hat{\boldsymbol{p}} =(\hat p_1, \ldots, \hat p_K) $ is close to 1 (and thus the remaining components are close to zero), then the prediction $\hat{\boldsymbol{p}}$ should be close to the prediction of Hinton's oracle which is provided by ${\boldsymbol{s}}_i$.

\subsection{Bayes covariant methods} \label{sec:bc2}

There are many Bayes covariant methods. The simplest example are arboreal coupling methods. They are not canonical, but with the help of ensembling, one can construct also canonical coupling methods. 

A very helpful aid to study Bayes covariant methods is property $BC_{ij}$ (the notation stands for \emph{Bayes covariant when restricted to classes} $i$ and $j$). A coupling method $\boldsymbol{v}$ satisfies $BC_{ij}$ if for any change of priors $\pi$ and any  pairwise data $\boldsymbol{R}$ the odds of classes $C_i$ to $C_j$ predicted by $\boldsymbol{v}(\boldsymbol{R}^\pi)$ and $\bigl(\boldsymbol{v}(\boldsymbol{R})\bigr)^\pi$ are equal. Using earlier notation for odds (see \eqref{eq:alpha}), one requires
\begin{align}
	\alpha_{ij} \bigl(\boldsymbol{v}(\boldsymbol{R}^\pi) \bigr) = \alpha_{ij} \bigl(\boldsymbol{v}(\boldsymbol{R})^\pi \bigr)
\end{align}

Let us list some easy to verify properties of this concept:

\begin{itemize}
	\item[a)] a Bayes covariant coupling method satisfies $BC_{ij}$ for any $i\not= j$ (see Proposition \ref{prop:bcprop} in Appendix \ref{app:bc1}),
	%\item[b)] if a coupling method satisfies $BC_{12}, BC_{23}, \ldots, BC_{(K-1),K}$ then it is Bayes covariant (see Proposition \ref{prop:bcprop}),
	\item[b)] conversely, if a coupling method satisfies $BC_{ij}$ for all $i\not= j$ then it is Bayes covariant (see Proposition \ref{prop:bcprop} in Appendix \ref{app:bc1}),
	\item[c)] if a coupling method satisfies $BC_{ij}$ and $BC_{jk}$ then it satisfies $BC_{ik}$ (see Lemma \ref{lem:transitivity} in Appendix \ref{app:bc1}).
\end{itemize}

Now we are ready to prove the our first key result.

\begin{prop}
Any arboreal method is Bayes covariant.
\end{prop}
\begin{proof}

An arboreal coupling method $\boldsymbol{t}$ induced by a spanning tree $T$ on the classes satisfies $BC_{ij}$ whenever the edge $C_iC_j$ belongs to $T$. Since any pair of vertices is connected by a path, it follows from transitivity of $BC_{ij}$ (property c)\,) that the assumption of b) holds and thus the coupling method is Bayes covariant.
	
\end{proof}

Given several Bayes covariant methods one can form  their ``linear combination'' - an ensemble which is also Bayes covariant. Let us make the construction explicit.

Suppose first that $a_1, \ldots,a_M$ are real numbers and $\boldsymbol{v}_1, \ldots, \boldsymbol{v}_M$ are probabilistic multi-class classification methods satisfying Assumption \ref{ass:2}. We define their linear combination $\bigoplus_i a_m \boldsymbol{v}_m$ as the classification method that for any $x$ yields the probability distribution on $K$ classes that satisfies
\begin{align*}
 	\bigl(a_1 \boldsymbol{v}_1 \oplus \ldots \oplus a_M \boldsymbol{v}_M\bigr)(x) \propto \boldsymbol{v}_1^{a_1}(x) \odot \cdots \odot \boldsymbol{v}_M^{a_M}(x),
\end{align*}
where the symbol $\odot$ denotes componentwise multiplication, and exponentials are computed component-wise. With this definition we can state the following result.

\begin{prop}
	\label{prop:ensemble}
Suppose the coupling methods $\boldsymbol{v}_1, \ldots, \boldsymbol{v}_M$ are Bayes covariant. If $\sum a_m = 1$, then $\boldsymbol{e} = \bigoplus_m a_m \boldsymbol{v}_m$ is also a Bayes covariant coupling method.
\end{prop}

\begin{proof}
It is sufficient to show that $BC_{ij}$ holds for any $i\not= j$. 

Let $\pi$ be a change of priors from $(P_1, \ldots, P_K)$  to $(P'_1, \ldots, P'_K)$. Let $o_m$ denotes the odds of classes $C_i$ and $C_j$ predicted by coupling method $\boldsymbol{v}_m$ i.e.
$$
o_m = \alpha_{ij}(\boldsymbol{v}_m(\boldsymbol{R})).
$$

From the definition of linear combination of coupling methods that the odds of $C_i$ to $C_j$ for $\boldsymbol{e}$ are
\begin{align}
\prod_m o_m^{a_m}.
\end{align}

It follows from \eqref{def:prior:effect} that the odds of $\boldsymbol{e}^\pi$ are 
\begin{align}
\frac{P'_i}{P_i} \frac{P_j}{P'_j} \prod_m o_m^{a_m}.
\end{align}

Let us now use Bayes covariance of $\boldsymbol{v}_i$. 
We have
\begin{align}
	\biggl(\bigoplus_m a_m \boldsymbol{v}_m\biggr)(\boldsymbol{R^\pi}) & \propto 
	\boldsymbol{v}_1^{a_1}(\boldsymbol{R}^\pi) \odot \cdots \odot \boldsymbol{v}_M^{a_M}(\boldsymbol{R}^\pi) =  \bigodot_m  (\boldsymbol{v}_m^\pi(\boldsymbol{R}))^{a_m}. \\
%	&= \frac{P'_i}{P_i} \frac{P_j}{P'_j} \prod_m o_m^{a_m}
\end{align}
Therefore the odds of $C_i$ to $C_j$ for $\bigoplus_m a_m \boldsymbol{v}_m$ are 
\begin{align}
\biggl(\frac{P'_i}{P_i} \frac{P_j}{P'_j} \biggr)^{\sum a_m} \prod_m o_m^{a_m}.
\end{align}

Clearly, under the assumption $\sum_m a_m = 1$, the classifier $\bigoplus_m a_m \boldsymbol{v}_m$ satisfies $BC_{ij}$. Since this holds for any $i\not=j$,  it follows that $\boldsymbol{e}$ is Bayes covariant.
\end{proof}


% 
% given such $\alpha + \beta = 1$. If the prediction of the two Bayes covariant methods $\boldsymbol{B}$ and $\boldsymbol{\tilde B}$ are
%\begin{align*}
%	\boldsymbol{B}(x) = (p_1, \ldots, p_K)\quad\textrm{and}\quad \boldsymbol{\tilde B}(x) = (\tilde p_1, \ldots, \tilde p_K)
%\end{align*}
%then the prediction of the ensemble $\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}$ is given by
%\begin{align*}
%	\bigl(\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}\bigr)(x)\propto (\,p_1^\alpha \tilde p_1^\beta, \ldots, p_K^\alpha \tilde p_K^\beta).
%\end{align*}

As an example, consider the classification problem with three classes. We have three arboreal classifiers $\boldsymbol{s}_1, \boldsymbol{s}_2, \boldsymbol{s}_3$ which correspond to star graphs centered respectively at classes $C_1, C_2, C_3$. The classifier 
\begin{align}
\boldsymbol{e}_3 \stackrel{def}{=} \frac13 \boldsymbol{s}_1 \oplus \frac13 \boldsymbol{s}_2 \oplus \frac 13 \boldsymbol{s}_3  \label{eq:normal3}
\end{align}
is Bayes covariant \emph{and} symmetric. A key result proved in \cite{vsuch2016bayes} is that it is a unique  classifier having these two properties.

\begin{thm} \label{thm:K3}
	There exists a unique symmetric Bayes covariant coupling methods for $K=3$.
\end{thm}

\begin{proof}
Let 
\begin{align}
	\boldsymbol{R}_s \stackrel{def}{=} \begin{pmatrix} \cdot & s & 1 -s \\  1-s & \cdot & s \\ s & 1-s & \cdot \end{pmatrix}
\end{align}
Since $R_s$ is invariant under the action of any permutation of 3 elements, 
for a symmetric coupling method we have 
\begin{align}
	\boldsymbol{v} (\boldsymbol{R}_s) = \biggl(\frac 13, \frac 13, \frac 13 \biggr).
\end{align}
Now applying  Proposition \ref{prop:bc3} from Appendix \ref{app:bc1} yields uniqueness. The existence of the method was already shown above using Proposition \ref{prop:ensemble}.
\end{proof}


Let us point out one consequence of the theorem. We could define another ensemble $\tilde{\boldsymbol{e}}$ by averaging the predictions of $\boldsymbol{s}_1, \boldsymbol{s}_2, \boldsymbol{s}_3$:
\[
\tilde{\boldsymbol{e}} = \frac13 \boldsymbol{s}_1(x) + \frac13 \boldsymbol{s}_2(x) + \frac 13 \boldsymbol{s}_3(x).
\]
Note that here we use the plus symbol to denote the vector addition of vectors representing  probability distributions over $K$ classes. This is a different classifier from the one given by  \eqref{eq:normal3} , and since it is symmetric, it cannot be Bayes covariant.

The classifier $\boldsymbol{e}_3$ has a canonical generalization for any $K>3$, namely the classifier
\begin{align}
\boldsymbol{e}_K \stackrel{def}{=} \frac1K \boldsymbol{s}_1 \oplus \frac1K \boldsymbol{s}_2 \oplus \ldots \oplus \frac 1K \boldsymbol{s}_K  \label{eq:bc1}
\end{align}
We call this coupling method the \emph{normal} coupling method.


