

\section{Examples of coupling methods} \label{sec:coupling}

There is a plethora of methods available for coupling. In this section we describe four major classes.

\subsection{Methods aiming to minimize binary divergence}

A multi-class estimate $\hat{\boldsymbol{p}}$ obtained by a coupling method implies via \eqref{eq:bt1} probability distributions on each pair of classes:
\begin{align*}
	\hat D_{ij}^\textrm{multi}= \biggl(\frac{\hat{p_i}}{\hat p_i + \hat p_j},\frac{\hat{p_j}}{\hat p_i + \hat p_j}\biggr)	
\end{align*}
On the other hand, binary classifiers provide another distribution for each pair of  classes, namely
\begin{align*}
\hat D_{ij}^\textrm{binary} = (\hat r_{ij}, \hat r_{ji}).
\end{align*}

A natural way to find  $\hat{\boldsymbol{p}}$ is thus to minimize a functional incorporating divergence measures between these distributions
\begin{align*}
\hat{\boldsymbol{p}} \stackrel{def}{=} \argmin_{\boldsymbol{p}} \sum_{ij} w_{ij} \divm (\hat D_{ij}^\textrm{multi}, \hat D_{ij}^\textrm{binary})
\end{align*}

A notable example which uses this principle is the coupling method introduced in [\cite{hastie1998classification}]. Their method uses Kullback-Leibler divergence $\divm_\textrm{KL}$
\begin{align*}
	\divm\nolimits_\textrm{KL} (\boldsymbol{p}, \boldsymbol{q})= \sum p_i \log (p_i/ q_i)
\end{align*}

Kullback-Leibler divergence strongly penalizes  mispredictions when the true class is predicted to have very low probability. In the context of classification, when we are primarily concerned with accuracy , one could advantageously use other Bregman divergence derived from a proper score [\cite{gneiting2007strictly, buja2005loss}].
 
For instance, a popular alternative is divergence arising from the quadratic  score [\cite{gneiting2007strictly}] (also known as Brier score [\cite{brier1950verification}]). This divergence takes the form
$$
\divm\nolimits_\textrm{quad} (\boldsymbol{p}, \boldsymbol{q})= \sum (p_i - q_i)^2.
$$
Unlike Kullback-Leibler divergence, the penalty for incorrect prediction with arbitrarily low probability is bounded (by one). This quadratic score is the basis of the popular Wu-Lin-Weng's method [\cite{wu2004probability}]. 

Their method uses an additional trick, which we term \emph{self-referentiality}. In the method of Hastie and Tibshirani, $w_{ij}$ are constants derived from observed class frequencies. However, we could use weights $w_{ij}$ that increase when the predicted probability of either class $i$ or $j$ is high. A reasonable choice would be for instance $w_{ij}= p_i + p_j$. In the case of quadratic score, it is more convenient to use weights $w_{ij} = (p_i + p_j)^2$ because then the resulting optimization problem is just minimization of a quadratic form
\begin{align*}
\hat{\boldsymbol{p}} \stackrel{def}{=} \argmin_{\boldsymbol{p}} \sum_{i,j} (r_{ij}p_j - r_{ji}p_i)^2.
\end{align*}

We note that the method is symmetric. The choice of quadratic scoring rule can also be viewed as canonical in view of axiomatic characterization given in  [\cite{selten1998axiomatic}].  However the choice of weights $w_{ij}$ to make the method computationally amenable is somewhat arbitrary. Thus we do not view Wu-Lin-Weng's method as canonical.

\subsection{Arboreal methods}

System of equations \eqref{eq:bt2} is overdetermined because there are $\binom{K}{2}$ equations together with the requirement that the total sum of predicted probabilities is one.  Given an overdetermined system of equations, a common approach is to select a minimal subset of equations needed to solve for the unknowns. This principle underlies arboreal coupling methods.

An arboreal coupling method ignores all but $K-1$ of the pairwise predictions. A simple example for 3 classes is the classifier which ignores prediction of the binary classifier distinguishing between classes 1 and 3. The two equations form \eqref{eq:bt2} together with the requirement that the prediction probabilities should sum to 1 yield the following system of equations
\begin{equation}
	\begin{split}
		\frac{\hat p_1}{\hat p_1 + \hat p_2} &= {r}_{12}\\
		\frac{\hat p_2}{\hat p_2 + \hat p_3} &= {r}_{23}\\
		\hat p_1 + \hat p_2 + \hat p_3 &= 1
	\end{split}
	\label{eq:arb1}
\end{equation}

For $K=3$, there are three different arboreal coupling methods, which we shall call $\boldsymbol{s}_1$, $\boldsymbol{s}_2$, $\boldsymbol{s}_3$, where the system \eqref{eq:arb1} represents the coupling method $\boldsymbol{s}_2$.

For $K>3$ the three methods can be generalized to corresponds to the trees having the graph structure  of a star centered at class $C_i$ for $i=1, \ldots, K$.

More generally, to obtain a regular system of equations for  any of $K$ , it is necessary that the tree corresponding to an arboreal method forms a spanning tree. Note that any arboreal method is non-canonical, because there is no natural way to choose a tree in a complete graph. In fact, a stronger statement is true -- no arboreal method is symmetric.
%However, we will see in Section \ref{sec:bc2} that by ``averaging'' several stars we can obtain a canonical 



\subsection{Methods emphasizing Hinton's minimality}

A variant on arboreal methods is the following oracle method which we shall call Hinton's oracle. It  assumes it has the access to the ground truth $y = C_t$. It outputs probabilities by solving only the equations
$$
\frac{\hat p_t}{\hat p_t + \hat p_j} = {r}_{tj},\quad \textrm{for }j\not = t,
$$
together the  with total sum equation
$$
\sum_{i=1}^K  \hat p_i = 1.
$$

For any sample, its result is identical to the arboreal method where the underlying tree forms a star. This method is clearly the optimal one with respect to Hinton's minimality criterion, because it ignores the output of any classifier not trained on the true class. It is not a true classification method, however, because it ``peeks'' at the label. Its main utility is to provide a way to benchmark other methods with respect to Hinton's minimality.

In order to convert Hinton's oracle into a true classification method we can use the principle of self-referentiality.  Denote by $\boldsymbol{s}_i$ the prediction of the arboreal method, where the underlying tree is a star centred at the class $C_i$. The predictions will generally span the vector space $R^K$. In that case the prediction $\hat{\boldsymbol{p}}$ of the method  can be expressed as a linear combination of $\boldsymbol{s}_i(x)$. Let us now define \emph{radial} coupling method. The  method requires that for the  weights we could in fact use the components of $\boldsymbol{p}$ i.e.
\begin{equation}
	\begin{split}
	\hat {\boldsymbol{p}} &= \hat p_1 \boldsymbol{s}_1(x) + \ldots + \hat p_K \boldsymbol{s}_K(x)\\
	1 &= \hat p_1 + \ldots + \hat p_K
	\end{split}
	 \label{eq:radial}
\end{equation}
%
The intuition behind \eqref{eq:radial} is that if some component of $\hat{\boldsymbol{p}}$ is close to 1 (and thus the remaining components are close to zero), then the prediction $\hat{\boldsymbol{p}}$ should be close to the prediction of Hinton's oracle which is provided by ${\boldsymbol{s}}_i$.

\subsection{Bayes covariant methods} \label{sec:bc2}

There are many Bayes covariant methods. In this section we will show that 
\begin{itemize} 
	\item arboreal coupling methods are Bayes covariant,
	\item Bayes covariant methods can be ensembled,
	\item describe all Bayes covariant methods for $K=3$.
\end{itemize}

We start by defining the property $BC_{ij}$ (the notation stands for \emph{Bayes covariant when restricted to classes} $i$ and $j$) of a coupling method. A coupling method $\boldsymbol{v}$ satisfies $BC_{ij}$ if for any change of priors $\pi$ and any  pairwise data $\boldsymbol{R}$ the odds of classes $C_i$ to $C_j$ predicted by $\boldsymbol{v}(\boldsymbol{R}^\pi)$ and $\bigl(\boldsymbol{v}(\boldsymbol{R})\bigr)^\pi$ are equal. 

Let us list some easy to verify properties of this concept:

\begin{itemize}
	\item[a)] a Bayes covariant coupling method satisfies $BC_{ij}$ for any $i\not= j$,
	\item[b)] if a coupling method satisfies $BC_{12}, BC_{23}, \ldots, BC_{(K-1),K}$ then it is Bayes covariant,
	\item[c)] if a coupling method satisfies $BC_{ij}$ for all $i\not= j$ then it is Bayes covariant,
	\item[d)] if a coupling method satisfies $BC_{ij}$ and $BC_{jk}$ then it satisfies $BC_{ik}$ (see Proposition \ref{lem:transitivity}).
\end{itemize}

Now we are ready to prove the our first result.

\begin{prop}
Any arboreal method is Bayes covariant.
\end{prop}
\begin{proof}

An arboreal coupling method $\boldsymbol{t}$ induced by a spanning tree $T$ on the classes satisfies $BC_{ij}$ whenever the edge $C_iC_j$ belongs to $T$. It follows using from b) and d) that the method is Bayes covariant.
	
	
%	
%	
%Let $\pi$ be a change of priors from $\Pi = (P_1, \ldots, P_K)$ to $(P'_1, \ldots, P'_K)$. We need to verify $\boldsymbol{v}(\boldsymbol{R}^\pi) =\bigl( \boldsymbol{v}(\boldsymbol{R})\bigr)$ for any pairwise matrix $\boldsymbol{R}$. |We will rephrase this in an equivalent way. Let 
%\begin{align}
%	\begin{split}
%	\boldsymbol{v}(\boldsymbol{R}^\pi) &= (v'_1, \ldots, v'_K) \\
%	\boldsymbol{v}(\boldsymbol{R}) &= (v_1, \ldots, v_K)
%	\end{split}
%\end{align}
%Suppose $\pi$ is a change of priors from $\Pi = (P_1, \ldots, P_K)$ to $\Pi' = (P'_1, \ldots, P'_K)$. If the prediction given $\Pi$ is $(t_1, \ldots, t_K)$ then we need to prove that
%\begin{align*}
%\boldsymbol{t}(R^\pi) = \bigl(t_1 \frac{P'_1}{P_1}, \ldots, t_K \frac{P'_K}{P_K}\bigr)
%\end{align*}
%	
%	We can proceed by induction. The statement is clear when $K=2$. Suppose the proposition is true for an integers $<K$ and we are given a spanning tree $T$ on $K$ vertices. Any tree has a vertex of degree one. We may assume that in the case of $T$ it is the vertex corresponding to $C_1$ and the unique edge incident to $C_1$ is $C_1C_2$.
%	
%	 Denote by $\boldsymbol{t}$ the prediction corresponding to $T$ and by $\boldsymbol{t}'$ the prediction corresponding to the subgraph without the vertex $C_1$. Let us write 
%	\begin{align*}
%	\boldsymbol{t}(x) &= (t_1, t_2, \ldots, t_K) \\
%	\boldsymbol{t'}(x) &= ( t'_2, \ldots, t'_K) 
%\end{align*}
%These vectors are related as follows:
%\begin{align}
%\begin{split}
%\frac{t_1}{t_1+t_2} &= r_{12} \\
%(t_2, \ldots, t_K) &\propto (t'_2, \ldots, t'_{K})
%\end{split} \label{eq:ind1}
%\end{align}
%
%Now consider the effect of change of priors $\Pi = (P_1, \ldots, P_K)$ to $\Pi' = (P'_1, \ldots, P'_K)$. Applying \eqref{eq:changePrior} to the multi-class prediction yields a vector 
%$$
%\boldsymbol{t}^{\Pi\rightarrow \Pi'} \propto (t_1 \frac{P'_1}{P_1}, \ldots, t_K \frac{P'_K}{P_K}).
%$$
%Now consider what happens when we apply \eqref{eq:changePrior} before coupling. By induction assumption the method $\boldsymbol{t}'$ is Bayes covariant and thus applying coupling \emph{after} change of priors yields
%$$
%\boldsymbol{t}'^{\Pi\rightarrow \Pi'} \propto (t_2 \frac{P'_2}{P_2}, \ldots, t_K \frac{P'_K}{P_K})
%$$

\end{proof}

Given several Bayes covariant methods one can form  their ``linear combination'' - an ensemble which is also Bayes covariant. Let us make it explicit.

Suppose real numbers $a_1, \ldots,a_M$ are real numbers and $\boldsymbol{v}_1, \ldots, \boldsymbol{v}_M$ are probabibilistic multi-class classification methods satisfying Assumption \ref{ass:1}. We define their linear combination $\bigoplus_i a_m \boldsymbol{v}_m$ as the classification method that for any $x$ yields the probability distribution on $K$ classes that satisfies
\begin{align*}
 	\bigl(a_1 \boldsymbol{v}_1 \oplus \ldots \oplus a_M \boldsymbol{v}_M\bigr)(x) \propto \boldsymbol{v}_1^{a_1}(x) \odot \cdots \odot \boldsymbol{v}_M^{a_M}(x),
\end{align*}
where the symbol $\odot$ denotes componentwise multiplication, and exponentials are computed component-wise. With this definition we can state the following result.

\begin{prop}
Suppose the coupling methods $\boldsymbol{v}_1, \ldots, \boldsymbol{v}_M$ are Bayes covariant. If $\sum a_m = 1$, then $\bigoplus_m a_m \boldsymbol{v}_m$ is also a Bayes covariant coupling method.
\end{prop}

\begin{proof}
We will be using property $BC_{ij}$ from the proof of the previous proposition. It is sufficient to show that $BC_{ij}$ holds for any $i\not= j$. 

Let $\pi$ be a change of priors from $(P_1, \ldots, P_K)$  to $(P'_1, \ldots, P'_K)$. Let $o_m$ denotes the odds of classes $C_i$ and $C_j$ predicted by coupling method $\boldsymbol{v}_m$. Then the odds of $C_i$ and $C_j$ predicted by $(\bigoplus_i a_i \boldsymbol{v}_i)^\pi$ are 
\begin{align}
	\biggl(\bigoplus_m a_m \boldsymbol{v}_m\biggr)^\pi(\boldsymbol{R}) & = \frac{P'_i}{P_i} \frac{P_j}{P'_j} \biggl(\bigoplus_m a_m \boldsymbol{v}_m\biggr)(\boldsymbol{R}) \\
\end{align}
so the odds are 

\begin{align}
	&= \frac{P'_i}{P_i} \frac{P_j}{P'_j} \prod_m o_m^{a_m}
\end{align}

Let us now use Bayes covariance of $\boldsymbol{v}_i$. 
We have
\begin{align}
	\biggl(\bigoplus_m a_m \boldsymbol{v}_m\biggr)(\boldsymbol{R^\pi}) & =  \biggl(\bigoplus_m a_m \boldsymbol{v}_m^\pi\biggr)(\boldsymbol{R}) \\
%	&= \frac{P'_i}{P_i} \frac{P_j}{P'_j} \prod_m o_m^{a_m}
\end{align}

It follows that the odds of $C_i$ to $C_j$ for $\bigoplus_m a_m \boldsymbol{v}_m$ are 
\begin{align}
\biggl(\frac{P'_i}{P_i} \frac{P_j}{P'_j} \biggr)^{\sum a_m} \prod_m o_m^{a_m}.
\end{align}
Clearly, under the assumption $\sum_m a_m = 1$, the classifier $\bigoplus_m a_m \boldsymbol{v}_m$ satisfies $BC_{ij}$ and it follows that it is Bayes covariant.
\end{proof}


% 
% given such $\alpha + \beta = 1$. If the prediction of the two Bayes covariant methods $\boldsymbol{B}$ and $\boldsymbol{\tilde B}$ are
%\begin{align*}
%	\boldsymbol{B}(x) = (p_1, \ldots, p_K)\quad\textrm{and}\quad \boldsymbol{\tilde B}(x) = (\tilde p_1, \ldots, \tilde p_K)
%\end{align*}
%then the prediction of the ensemble $\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}$ is given by
%\begin{align*}
%	\bigl(\alpha 	\boldsymbol{B} \oplus \beta \boldsymbol{\tilde B}\bigr)(x)\propto (\,p_1^\alpha \tilde p_1^\beta, \ldots, p_K^\alpha \tilde p_K^\beta).
%\end{align*}

As an example, consider the classification problem with three classes. We have three arboreal classifiers $\boldsymbol{s}_1, \boldsymbol{s}_2, \boldsymbol{s}_3$ which correspond to star graphs centered respectively at classes $C_1, C_2, C_3$. The classifier 
\begin{align}
\frac13 \boldsymbol{s}_1 \oplus \frac13 \boldsymbol{s}_2 \oplus \frac 13 \boldsymbol{s}_3  \label{eq:bc1}
\end{align}
is Bayes covariant \emph{and} symmetric. A key result proved in [\cite{vsuch2016bayes}] is that it is a unique such classifier for $K=3$.

\begin{thm} \label{thm:K3}
	There exists a unique symmetric Bayes covariant coupling methods for $K=3$.
\end{thm}

Let us point out one consequence of the theorem. We could define another ensemble $\boldsymbol{E}$ by averaging the predictions of $\boldsymbol{s}_1, \boldsymbol{s}_2, \boldsymbol{s}_3$:
\[
\boldsymbol{E} = \frac13 \boldsymbol{s}_1(x) + \frac13 \boldsymbol{s}_2(x) + \frac 13 \boldsymbol{s}_3(x).
\]
Note that here we use the plus symbol to denote the vector addition of vectors representing $K$-class probability distributions. This is a different classifier from the one given by  \eqref{eq:bc1} (see Appendix \ref{app:explicit}), and since it is symmetric, it cannot be Bayes covariant.

\subsection{Properties of coupling methods}


The following table indicates which properties are enjoyed by each coupling method we described in this section.

\begin{table}[!ht]
\begin{tabular}{cm{2.5cm}m{1.5cm}m{1.5cm}ccm{1.5cm}m{1.5cm}}
&property & Hastie-Tibshirani & Wu-Lin-Weng & radial & normal & arboreal & Hinton's oracle \\
\hline 
\multirow{5}{*}{\begin{turn}{90}\makecell{exact}\end{turn}}
&uniqueness &  yes & yes & yes & yes & yes & yes \\
&Bradley-Terry consistency & yes & yes & yes & yes & yes & yes \\
&canonicity & yes & no & yes & yes & no & yes \\
&symmetry & yes & yes & yes & yes & no & yes \\
& Bayes covariant & no & no & no & yes & yes & yes \\
\hline
\multirow{5}{*}{\begin{turn}{90}\makecell{non-exact}\end{turn}}
&Hinton's minimality & ? & ?  & ++ & ?  & ? & +++ \\
&lack of parameters & yes & yes & yes & yes & yes & yes \\
& amounts to a linear system & no & yes & yes & yes & yes & yes\\
& linear system of special form & no & no & yes & yes & yes & yes \\
& iterative method & yes & yes & yes & no & not needed & not needed\\
\hline
\end{tabular}
\caption{Summary of properties of coupling methods described in Section \ref{sec:coupling}.}
\label{tab:summaryCoupling}
\end{table}

